{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "//Panduan Paling Komprehensif untuk K-Means Clustering yang Pernah diButuhkan//\n",
    "-* Gambaran *- \n",
    "• K-Means Clustering adalah algoritme sederhana namun kuat dalam ilmu data.\n",
    "• Ada banyak sekali aplikasi K-Means Clustering di dunia nyata. \n",
    "• Panduan komprehensif ini akan memperkenalkan kita pada dunia clustering dan K-Means Clustering bersama dengan implementasi dengan Python pada dataset dunia nyata.\n",
    "\n",
    "-* Pengantar *-\n",
    "    Saya suka bekerja di mesin rekomendasi. Setiap kali saya menemukan mesin rekomendasi apa pun di situs web, saya tidak sabar untuk memecahnya dan memahami cara kerjanya di bawahnya. Itu adalah salah satu dari banyak hal hebat tentang menjadi ilmuwan data.\n",
    "    Yang benar-benar membuat saya terpesona tentang sistem ini adalah bagaimana kita dapat mengelompokkan item, produk, dan pengguna yang serupa. Pengelompokan ini, atau segmentasi, bekerja lintas industri. Dan itulah yang membuat konsep pengelompokan menjadi penting dalam ilmu data.\n",
    "    - Pengelompokan membantu kami memahami data kami dengan cara yang unik\n",
    "    - dengan mengelompokkan berbagai hal menjadi \n",
    "    - dapat menebaknya\n",
    "    – dikerjakan secara berkelompok\n",
    "    \n",
    "    Berukut adalah hal- hal yang akan kita bahas dalam Pembelajaran \n",
    "    Daftar Isi\n",
    "    1. Apa itu Clustering?\n",
    "    2. Bagaimana Pengelompokan Masalah Pembelajaran Tanpa Pengawasan?\n",
    "    3. Properti Cluster\n",
    "    4. Aplikasi Clustering dalam Skenario Dunia Nyata\n",
    "    5. Memahami Metrik Evaluasi yang Berbeda untuk Clustering\n",
    "    6. Apa itu K-Means Clustering?\\n\",\n",
    "    7. Menerapkan K-Means Clustering dari awal dengan Python\n",
    "    8. Tantangan dengan Algoritma K-Means\\n\",\n",
    "    9. K-Means ++ untuk memilih centroid cluster awal untuk K-Means Clustering\n",
    "    10. Bagaimana cara memilih Jumlah Cluster yang Tepat di K-Means?\n",
    "    11. Menerapkan K-Means Clustering dengan Python\n",
    "dan akan dibahas secara berkala \n",
    "-* 1. Apa itu Clustering ? *- \n",
    "\n",
    "   Mari kita mulai dengan contoh sederhana. Bank ingin memberikan penawaran kartu kredit kepada nasabahnya. Saat ini, mereka melihat detail setiap pelanggan dan berdasarkan informasi ini, memutuskan penawaran mana yang harus diberikan kepada pelanggan mana.\n",
    "    Sekarang, bank berpotensi memiliki jutaan nasabah. Apakah masuk akal untuk melihat detail setiap pelanggan secara terpisah dan kemudian membuat keputusan? Tentu tidak! Ini adalah proses manual dan akan memakan banyak waktu.\n",
    "    Jadi, apa yang bisa dilakukan bank? Salah satu opsinya adalah menyegmentasikan pelanggannya ke dalam kelompok yang berbeda. Misalnya, bank dapat mengelompokkan nasabah berdasarkan pendapatannya\n",
    "    Dapatkah Anda melihat ke mana saya akan pergi dengan ini? Bank sekarang dapat membuat tiga strategi atau penawaran berbeda, satu untuk setiap kelompok. Di sini, alih-alih membuat strategi berbeda untuk pelanggan individu, mereka hanya perlu membuat 3 strategi. Ini akan mengurangi tenaga serta waktu.\n",
    "    Grup yang saya tunjukkan di atas dikenal sebagai cluster dan proses pembuatan grup ini dikenal sebagai clustering. Secara formal, kita dapat mengatakan bahwa Clustering adalah proses membagi seluruh data menjadi beberapa kelompok (disebut juga cluster) berdasarkan pola dalam data. Dapatkah Anda menebak jenis pengelompokan masalah pembelajaran itu? Apakah ini masalah pembelajaran yang diawasi atau tidak diawasi? Pikirkan sejenak dan manfaatkan contoh yang baru saja kita lihat. Oke? Pengelompokan adalah masalah belajar tanpa pengawasan\n",
    " Bagaimana Mengelompokkan Masalah Pembelajaran Tanpa Pengawasan?\n",
    "    \"Misalkan Anda sedang mengerjakan sebuah proyek di mana Anda perlu memprediksi penjualan pasar besar Atau, proyek di mana tugas Anda adalah memprediksi apakah pinjaman akan disetujui atau tidak.\n",
    "   Kami memiliki target tetap untuk diprediksi dalam kedua situasi ini. Pada soal prediksi penjualan, kita harus memprediksi Item_Outlet_Sales berdasarkan outlet_size, outlet_location_type, dll. Dan dalam masalah loan approval, kita harus memprediksi Loan_Status berdasarkan Gender, status perkawinan, pendapatan pelanggan, dll.\n",
    "  Jadi, ketika kita memiliki variabel target untuk diprediksi berdasarkan sekumpulan prediktor atau variabel independen tertentu, masalah seperti itu disebut masalah pembelajaran yang diawasi. Sekarang, mungkin ada situasi di mana kami tidak memiliki variabel target untuk diprediksi. Masalah seperti itu, tanpa variabel target tetap, dikenal sebagai masalah pembelajaran tanpa pengawasan. Dalam masalah ini, kami hanya memiliki variabel independen dan tidak ada variabel target dependen.\n",
    "  Dalam clustering, kami tidak memiliki target untuk diprediksi. Kami melihat data dan kemudian mencoba untuk mengumpulkan pengamatan serupa dan membentuk kelompok yang berbeda. Oleh karena itu, ini adalah masalah belajar yang tidak diawasi. Sekarang kita tahu apa itu cluster dan konsep clustering. Selanjutnya, mari kita lihat properti dari cluster ini yang harus kita pertimbangkan saat membentuk cluster.\n",
    "  \n",
    "  -* Properti Cluster *-\n",
    "    Bagaimana dengan contoh lainnya? Kami akan mengambil bank yang sama seperti sebelumnya yang ingin menyegmentasikan pelanggannya. Untuk kesederhanaan, katakanlah bank hanya ingin menggunakan pendapatan dan hutang untuk membuat segmentasi. Mereka mengumpulkan data pelanggan dan menggunakan plot pencar untuk memvisualisasikannya.\n",
    "    Pada sumbu X, kami memiliki pendapatan pelanggan dan sumbu y mewakili jumlah hutang. Di sini, kami dapat dengan jelas memvisualisasikan bahwa pelanggan ini dapat dibagi menjadi 4 cluster yang berbeda seperti yang ditunjukkan di bawah ini. \n",
    "    Beginilah cara pengelompokan membantu membuat segmen (cluster) dari data. Bank selanjutnya dapat menggunakan cluster ini untuk membuat strategi dan menawarkan diskon kepada nasabahnya. Jadi mari kita lihat properti dari cluster ini.\n",
    "    \n",
    "   // Properti 1 //\n",
    "    Semua titik data dalam sebuah cluster harus serupa satu sama lain. Izinkan saya mengilustrasikannya menggunakan contoh di atas. Jika pelanggan di cluster tertentu tidak mirip satu sama lain, maka persyaratan mereka mungkin berbeda, bukan? Jika bank memberi mereka penawaran yang sama, mereka mungkin tidak menyukainya dan minat mereka pada bank mungkin berkurang. Tidak ideal.\n",
    "    Memiliki titik data serupa dalam cluster yang sama membantu bank untuk menggunakan pemasaran yang ditargetkan. Anda dapat memikirkan contoh serupa dari kehidupan sehari-hari Anda dan memikirkan tentang bagaimana pengelompokan akan (atau sudah) memengaruhi strategi bisnis.\n",
    "\n",
    "   // Properti 2 //\n",
    "    Titik data dari cluster yang berbeda harus dibuat berbeda sebisa mungkin. Ini secara intuitif akan masuk akal jika Anda memahami properti di atas. Mari kita ambil lagi contoh yang sama untuk memahami properti ini.\n",
    "Manakah dari kasus berikut yang menurut Anda akan memberi kami cluster yang lebih baik? Jika Anda melihat kasus \n",
    "    Pelanggan dalam kelompok merah dan biru sangat mirip satu sama lain. Empat poin teratas di cluster merah memiliki properti yang serupa dengan dua pelanggan teratas di cluster biru. Mereka memiliki pendapatan tinggi dan nilai hutang yang tinggi. Di sini, kami mengelompokkan mereka secara berbeda. Sedangkan jika melihat kasus II. \n",
    "    Poin di cluster merah sama sekali berbeda dari pelanggan di cluster biru. Semua pelanggan di cluster merah memiliki pendapatan tinggi dan hutang tinggi dan pelanggan di cluster biru memiliki pendapatan tinggi dan nilai hutang rendah. Jelas kami memiliki pengelompokan pelanggan yang lebih baik dalam kasus ini.\\n\",\n",
    "    Oleh karena itu, poin data dari cluster yang berbeda harus sama berbeda satu sama lain mungkin untuk memiliki cluster yang lebih bermakna. Sejauh ini, kami telah memahami apa itu clustering dan berbagai properti cluster. Tetapi mengapa kita bahkan membutuhkan pengelompokan? Mari kita perjelas keraguan ini di bagian selanjutnya dan lihat beberapa aplikasi pengelompokan.\n",
    "    Aplikasi Clustering dalam Skenario Dunia Nyata Pengelompokan adalah teknik yang banyak digunakan di industri. Ini sebenarnya digunakan di hampir setiap domain, mulai dari perbankan hingga mesin rekomendasi, pengelompokan dokumen hingga segmentasi gambar. \n",
    "    Segmentasi pelanggan Kami membahas ini sebelumnya - salah satu aplikasi pengelompokan yang paling umum adalah segmentasi pelanggan. Dan itu tidak hanya terbatas pada perbankan. Strategi ini lintas fungsi, termasuk telekomunikasi, e-commerce, olahraga, periklanan, penjualan, dll.\n",
    "   \n",
    "   Pengelompokan Dokumen Ini adalah aplikasi umum pengelompokan lainnya. Katakanlah Anda memiliki banyak dokumen dan Anda perlu mengelompokkan dokumen serupa. Clustering membantu kami mengelompokkan dokumen-dokumen ini sedemikian rupa sehingga dokumen serupa berada dalam cluster yang sama.\n",
    "    Segmentasi Gambar Kita juga bisa menggunakan clustering untuk melakukan segmentasi gambar. Di sini, kami mencoba menggabungkan piksel serupa pada gambar menjadi satu. Kita dapat menerapkan clustering untuk membuat cluster yang memiliki piksel serupa di grup yang sama.Anda dapat merujuk ke artikel ini untuk melihat bagaimana kita dapat menggunakan pengelompokan untuk tugas segmentasi gambar.\n",
    "    \n",
    "   Mesin Rekomendasi. \n",
    "   Clustering juga dapat digunakan di mesin rekomendasi. Misalnya Anda ingin merekomendasikan lagu ke teman Anda. Anda dapat melihat lagu-lagu yang disukai oleh orang itu dan kemudian menggunakan pengelompokan untuk menemukan lagu yang serupa dan akhirnya merekomendasikan lagu yang paling mirip.\n",
    "    Masih banyak lagi aplikasi yang saya yakin sudah Anda pikirkan. Anda dapat membagikan aplikasi ini di bagian komentar di bawah. Selanjutnya, mari kita lihat bagaimana kita dapat mengevaluasi cluster kita.\n",
    "    Memahami Berbagai Metrik Evaluasi untuk Pengelompokan\n",
    "    Tujuan utama clustering tidak hanya untuk membuat cluster, tetapi untuk membuat cluster yang baik dan bermakna. Kami melihat ini pada contoh di bawah ini:\n",
    "    Di sini, kami hanya menggunakan dua fitur dan karenanya mudah bagi kami untuk memvisualisasikan dan memutuskan cluster mana yang lebih baik.\n",
    "    Sayangnya, skenario dunia nyata tidak bekerja seperti itu. Kami akan memiliki banyak fitur untuk dikerjakan. Mari kita ambil contoh segmentasi pelanggan lagi - kita akan memiliki fitur seperti pendapatan pelanggan, pekerjaan, jenis kelamin, usia, dan banyak lagi. Memvisualisasikan semua fitur ini bersama-sama dan memutuskan cluster yang lebih baik dan bermakna tidak akan mungkin bagi kami.\n",
    "    Di sinilah kita dapat menggunakan metrik evaluasi. Mari kita bahas beberapa di antaranya dan pahami bagaimana kita dapat menggunakannya untuk mengevaluasi kualitas cluster kita.\n",
    "\n",
    "   Kelembaman\n",
    "    Ingat kembali properti pertama dari cluster yang kita bahas di atas. Inilah yang mengevaluasi inersia. Ini memberi tahu kita seberapa jauh titik dalam sebuah cluster. Jadi, inersia sebenarnya menghitung jumlah jarak semua titik dalam sebuah cluster dari pusat cluster tersebut.\n",
    "    Kami menghitung ini untuk semua cluster dan nilai inersia akhir adalah jumlah dari semua jarak ini. Jarak dalam cluster ini dikenal sebagai jarak intracluster. Jadi, inersia memberi kita jumlah jarak intracluster. Sekarang, menurut Anda apa yang seharusnya menjadi nilai inersia untuk cluster yang baik? Apakah nilai inersia kecil baik atau kita membutuhkan nilai yang lebih besar? Kami ingin poin dalam cluster yang sama serupa satu sama lain, bukan? Oleh karena itu, jarak di antara mereka harus serendah mungkin.\n",
    "    Dengan mengingat hal ini, kita dapat mengatakan bahwa semakin rendah nilai inersia, semakin baik cluster kita.\n",
    "    \n",
    "  // Indeks Dunn //\n",
    "    Kita sekarang tahu bahwa inersia mencoba meminimalkan jarak intracluster. Itu mencoba membuat cluster lebih kompak. Biar saya begini - jika jarak antara sentroid sebuah cluster dan titik-titik di cluster itu kecil, itu berarti titik-titiknya lebih dekat satu sama lain. Jadi, inersia memastikan bahwa properti pertama cluster terpenuhi. Tapi itu tidak peduli dengan properti kedua - bahwa cluster yang berbeda harus berbeda satu sama lain sebisa mungkin.\n",
    "    Di sinilah indeks Dunn dapat beraksi. Seiring dengan jarak antara centroid dan titik, indeks Dunn juga memperhitungkan jarak antara dua cluster. Jarak antara sentroid dari dua cluster yang berbeda ini dikenal sebagai jarak antar cluster. Mari kita lihat rumus indeks Dunn. \n",
    "    Indeks Dunn adalah rasio jarak antar cluster minimum dan jarak intracluster maksimum.\n",
    "    Kami ingin memaksimalkan indeks Dunn. Semakin tinggi nilai indeks Dunn, semakin baik klasternya. Mari kita pahami intuisi di balik indeks Dunn. Untuk memaksimalkan nilai indeks Dunn, pembilangnya harus maksimal. Di sini, kami mengambil jarak antar cluster minimum. Jadi, jarak antara cluster bahkan yang terdekat harus lebih jauh yang pada akhirnya akan memastikan bahwa cluster tersebut jauh satu sama lain.\n",
    "    Selain itu, penyebut harus minimum untuk memaksimalkan indeks Dunn. Di sini, kami mengambil jarak intracluster maksimum. Sekali lagi, intuisinya sama di sini. Jarak maksimum antara sentroid cluster dan titik-titik harus minimum yang pada akhirnya akan memastikan bahwa cluster kompak.\n",
    "    Ingat properti pertama dari cluster - ini menyatakan bahwa titik-titik dalam cluster harus serupa satu sama lain. Jadi, tujuan kami di sini adalah meminimalkan jarak antara titik-titik dalam sebuah cluster. Ada algoritma yang mencoba meminimalkan jarak titik-titik dalam sebuah cluster dengan centroidnya - teknik clustering k-means.\n",
    "    K-means adalah algoritme berbasis sentroid, atau algoritme berbasis jarak, tempat kami menghitung jarak untuk menetapkan titik ke sebuah cluster. Di K-Means, setiap cluster dikaitkan dengan sentroid. \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kami memiliki 8 poin ini dan kami ingin menerapkan k-means untuk membuat cluster untuk poin-poin ini. Inilah cara kami melakukannya. \n",
    "\n",
    "//Langkah 1: Pilih jumlah cluster //\n",
    "    Langkah pertama dalam k-means adalah memilih jumlah cluster,\n",
    "    \n",
    "//Langkah 2: Pilih k titik acak dari data sebagai sentroid//\n",
    "    Selanjutnya, kami secara acak memilih sentroid untuk setiap cluster. Misalkan kita ingin memiliki 2 cluster, jadi k sama dengan 2 di sini. Kami kemudian secara acak memilih sentroid:\\n\", Di sini, lingkaran merah dan hijau mewakili pusat massa untuk kelompok ini.\n",
    "    \n",
    "//Langkah 3: Tetapkan semua poin ke pusat cluster terdekat//\n",
    "    Setelah kami menginisialisasi sentroid, kami menetapkan setiap titik ke sentroid cluster terdekat Di sini Anda dapat melihat bahwa titik-titik yang lebih dekat ke titik merah ditetapkan ke kelompok merah sedangkan titik-titik yang lebih dekat ke titik hijau ditetapkan ke kelompok hijau.\n",
    "    \n",
    "//Langkah 4: Hitung ulang centroid dari cluster yang baru terbentuk//\n",
    "    Sekarang, setelah kita menetapkan semua titik ke salah satu cluster, langkah selanjutnya adalah menghitung centroid dari cluster yang baru terbentuk. Di sini, persilangan merah dan hijau adalah sentroid baru.\n",
    "    \n",
    "//Langkah 5: Ulangi langkah 3 dan 4//\n",
    "    Kami kemudian mengulangi langkah 3 dan 4. Langkah menghitung sentroid dan menetapkan semua titik ke kluster berdasarkan jaraknya dari sentroid adalah satu iterasi. Tapi tunggu - kapan kita harus menghentikan proses ini? Itu tidak bisa berjalan sampai keabadian, bukan?\n",
    "    Stopping Criteria for K-Means Clustering. Pada dasarnya ada tiga kriteria penghentian yang dapat diadopsi untuk menghentikan algoritma K-means:\n",
    "    1. Sentroid dari cluster yang baru terbentuk tidak berubah.\n",
    "    2. Poin tetap berada di cluster yang sama.\n",
    "    3. Jumlah iterasi maksimum tercapai. \n",
    "    Kami dapat menghentikan algoritme jika sentroid dari cluster yang baru terbentuk tidak berubah. Bahkan setelah beberapa iterasi, jika kita mendapatkan centroid yang sama untuk semua cluster, kita dapat mengatakan bahwa algoritme tersebut tidak mempelajari pola baru apa pun dan itu adalah tanda untuk menghentikan pelatihan.Tanda jelas lainnya bahwa kita harus menghentikan proses pelatihan jika poin tetap berada dalam cluster yang sama bahkan setelah melatih algoritme untuk beberapa iterasi.\n",
    "    Akhirnya, kita bisa menghentikan pelatihan jika jumlah iterasi maksimum tercapai. Misalkan kita telah menyetel jumlah iterasi sebagai 100. Proses akan berulang sebanyak 100 iterasi sebelum berhenti.\n",
    "    Implementing K-Means Clustering in Python from Scratch. Saatnya untuk menjalankan notebook Jupyter kami (atau IDE mana pun yang Anda gunakan) dan mengotori tangan kami dengan Python! Kami akan mengerjakan kumpulan data prediksi pinjaman yang dapat Anda unduh di sini. Saya mendorong Anda untuk membaca lebih lanjut tentang dataset dan pernyataan masalahnya di sini. Ini akan membantu Anda memvisualisasikan apa yang sedang kami kerjakan (dan mengapa kami melakukan ini). Dua pertanyaan yang cukup penting dalam proyek sains data.\n",
    "    Langkah 1 dan 2 dari K-Means adalah tentang memilih jumlah cluster (k) dan memilih sentroid acak untuk setiap cluster. Kami akan memilih 3 cluster dan kemudian memilih pengamatan acak dari data sebagai sentroid:\n",
    "    Nilai-nilai ini mungkin berbeda setiap kali kami menjalankan ini. Di sini, kami menghentikan pelatihan ketika sentroid tidak berubah setelah dua iterasi. Kami awalnya mendefinisikan diff sebagai 1 dan di dalam while loop, kami menghitung perbedaan ini sebagai perbedaan antara sentroid pada iterasi sebelumnya dan iterasi saat ini.\n",
    "    Jika selisihnya adalah 0, kami menghentikan pelatihan. Sekarang mari kita visualisasikan cluster yang kita punya. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-3-718f6055c148>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-718f6055c148>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    for k in range(K);\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "tcolor=['blue','green','cyan']\n",
    "    for k in range(K);\n",
    "        data=X[X[\\\"Cluster\\\"]==k+1];\n",
    "        plt.scatter(data[\\\"ApplicantIncome\\\"],data[\\\"LoanAmount\\\"],c=color[k]);\n",
    "    plt.scatter(Centroids[\\\"ApplicantIncome\\\"],Centroids[\\\"LoanAmount\\\"],c='red');\n",
    "    plt.xlabel('Income');\n",
    "    plt.ylabel('Loan Amount (In Thousands)');\n",
    "    plt.show();\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// Challenges with the K-Means Clustering Algorithm //\n",
    "    Salah satu tantangan umum yang kami hadapi saat bekerja dengan K-Means adalah ukuran cluster yang berbeda. Cluster paling kiri dan paling kanan berukuran lebih kecil dibandingkan dengan cluster pusat. Sekarang, jika kita menerapkan pengelompokan k-means pada titik-titik ini, hasilnya akan seperti ini.\n",
    "    Tantangan lain dengan k-means adalah ketika kepadatan titik awal berbeda. Katakanlah ini adalah poin aslinya.Di sini, titik-titik di cluster merah tersebar sedangkan titik-titik di cluster yang tersisa saling berdekatan. Sekarang, jika kita menerapkan k-means pada titik-titik ini, kita akan mendapatkan cluster seperti ini.\n",
    "    Kita dapat melihat bahwa titik kompak telah ditetapkan ke satu cluster. Sedangkan titik-titik yang tersebar secara longgar tetapi berada di cluster yang sama, telah ditetapkan ke cluster yang berbeda. Tidak ideal jadi apa yang bisa kita lakukan. Salah satu solusinya adalah dengan menggunakan jumlah cluster yang lebih banyak. Jadi, dalam semua skenario di atas, daripada menggunakan 3 cluster, kita bisa mendapatkan angka yang lebih besar. Mungkin pengaturan k = 10 dapat menghasilkan cluster yang lebih bermakna.\n",
    "    Ingat bagaimana kita menginisialisasi sentroid secara acak di pengelompokan k-means? Nah, ini juga berpotensi bermasalah karena kita mungkin mendapatkan cluster yang berbeda setiap saat. Jadi, untuk mengatasi masalah inisialisasi acak ini, ada algoritma bernama K-Means ++ yang dapat digunakan untuk memilih nilai awal, atau centroid cluster awal, untuk K-Means.\n",
    "    Masalah inisialisasi acak, ada algoritma bernama K-Means ++ yang dapat digunakan untuk memilih nilai awal, atau centroid cluster awal, untuk K-Means\n",
    "    K-Means++ to Choose Initial Cluster Centroids for K-Means Clustering. Dalam beberapa kasus, jika inisialisasi cluster tidak sesuai, K-Means dapat menghasilkan cluster yang buruk. Di sinilah K-Means ++ membantu. Ini menentukan prosedur untuk menginisialisasi pusat cluster sebelum melanjutkan dengan algoritma pengelompokan k-means standar.\n",
    "    Dengan menggunakan algoritme K-Means ++, kami mengoptimalkan langkah di mana kami memilih sentroid cluster secara acak. Kami lebih cenderung menemukan solusi yang kompetitif untuk solusi K-Means yang optimal saat menggunakan inisialisasi K-Means.\n",
    "    Langkah-langkah untuk menginisialisasi centroid menggunakan K-Means ++ adalah:\n",
    "    1. Cluster pertama dipilih secara acak dari titik-titik data yang ingin kita cluster. Ini mirip dengan apa yang kami lakukan di K-Means, tetapi alih-alih memilih semua sentroid secara acak, kami hanya memilih satu sentroid di sini. \n",
    "    2. Selanjutnya kita menghitung jarak (D (x)) setiap data point (x) dari center cluster yang sudah dipilih.\n",
    "    3. Kemudian, pilih pusat cluster baru dari titik data dengan probabilitas x sebanding dengan (D (x)) 2.\n",
    "    4. Kemudian ulangi langkah 2 dan 3 sampai k cluster telah dipilih.\n",
    "    Mari kita ambil contoh untuk memahami ini lebih jelas. Katakanlah kami memiliki poin-poin berikut dan kami ingin membuat 3 kelompok di sini, Sekarang, langkah pertama adalah memilih titik data secara acak sebagai pusat cluster.\n",
    "    Misalkan kita memilih titik hijau sebagai sentroid awal. Sekarang, kita akan menghitung jarak (D (x)) dari setiap titik data dengan centroid ini. Sentroid berikutnya adalah yang jarak kuadratnya (D (x) 2) adalah yang terjauh dari sentroid saat ini. Dalam hal ini, titik merah akan dipilih sebagai centroid berikutnya. Sekarang, untuk memilih centroid terakhir, kita akan mengambil jarak setiap titik dari centroid terdekatnya dan titik yang memiliki jarak kuadrat terbesar akan dipilih sebagai centroid berikutnya.\n",
    "    Kita dapat melanjutkan algoritme K-Means setelah menginisialisasi sentroid. Menggunakan K-Means ++ untuk menginisialisasi sentroid cenderung meningkatkan cluster. Meskipun secara komputasi mahal relatif terhadap inisialisasi acak, K-Means berikutnya sering kali bertemu lebih cepat. Saya yakin ada satu pertanyaan yang Anda tanyakan sejak awal artikel ini - berapa banyak kelompok yang harus kita buat? Aka, berapa jumlah cluster yang harus dimiliki saat menjalankan K-Means?\n",
    "    \n",
    "  -- How to Choose the Right Number of Clusters in K-Means Clustering? --\n",
    "  Salah satu keraguan paling umum yang dimiliki setiap orang saat bekerja dengan K-Means adalah memilih jumlah cluster yang tepat. Jadi, mari kita lihat teknik yang akan membantu kita memilih nilai cluster yang tepat untuk algoritme K-Means. Mari kita ambil contoh segmentasi pelanggan yang telah kita lihat sebelumnya. Singkatnya, bank ingin menyegmentasikan nasabahnya berdasarkan pendapatan dan jumlah utangnya.\n",
    "    Di sini, kami dapat memiliki dua cluster yang akan memisahkan pelanggan seperti yang ditunjukkan di bawah ini.\n",
    "Semua pelanggan dengan pendapatan rendah berada dalam satu cluster sedangkan pelanggan dengan pendapatan tinggi berada di cluster kedua. Kami juga dapat memiliki 4 cluster.\n",
    "    Di satu cluster dapat mewakili pelanggan yang berpenghasilan rendah dan hutang rendah, cluster lainnya adalah tempat pelanggan berpenghasilan tinggi dan hutang tinggi, dan sebagainya. Bisa juga ada 8 cluster.\n",
    "    Sejujurnya, kami dapat memiliki sejumlah cluster. Dapatkah Anda menebak berapa jumlah maksimum kluster yang mungkin? Satu hal yang dapat kita lakukan adalah menetapkan setiap titik ke cluster terpisah. Oleh karena itu, dalam hal ini jumlah cluster akan sama dengan jumlah titik atau pengamatan. Begitu, \n",
    "    Jumlah cluster maksimum yang mungkin akan sama dengan jumlah observasi dalam dataset.\n",
    "    Tapi lalu bagaimana kita bisa menentukan jumlah cluster yang optimal? Satu hal yang dapat kita lakukan adalah memplot grafik, juga dikenal sebagai kurva siku, di mana sumbu x akan mewakili jumlah cluster dan sumbu y akan menjadi metrik evaluasi. Katakanlah inersia untuk saat ini.\n",
    "    Anda juga dapat memilih metrik evaluasi lainnya seperti indeks Dunn. Selanjutnya, kita akan mulai dengan nilai cluster kecil, katakanlah 2. Latih model menggunakan 2 cluster, hitung inersia untuk model tersebut, dan terakhir plot di grafik di atas. Misalkan kita mendapatkan nilai inersia sekitar 1000.\n",
    "    Sekarang, kita akan menambah jumlah cluster, melatih model lagi, dan memplot nilai inersia. Inilah plot yang kami dapatkan. Ketika kami mengubah nilai cluster dari 2 menjadi 4, nilai inersia berkurang sangat tajam. Penurunan nilai inersia ini berkurang dan akhirnya menjadi konstan saat kami menambah jumlah cluster lebih lanjut.\n",
    "    Begitu, nilai cluster dimana penurunan nilai inersia ini menjadi konstan dapat dipilih sebagai nilai cluster yang tepat untuk data kita.\n",
    "    Di sini, kita dapat memilih sejumlah cluster antara 6 dan 10. Kita dapat memiliki 7, 8, atau bahkan 9 cluster. Anda juga harus melihat biaya komputasi sambil menentukan jumlah cluster. Jika kita menambah jumlah cluster, maka biaya komputasi juga akan meningkat. Jadi, jika Anda tidak memiliki sumber daya komputasi yang tinggi, saran saya adalah memilih jumlah cluster yang lebih sedikit.\n",
    "    Sekarang mari kita terapkan algoritme Pengelompokan K-Means dengan Python. Kita juga akan melihat bagaimana menggunakan K-Means ++ untuk menginisialisasi centroid dan juga akan memplot kurva siku ini untuk memutuskan berapa jumlah cluster yang tepat untuk dataset kita.\n",
    "    Implementing K-Means Clustering in Python\n",
    "    Kami akan menangani masalah segmentasi pelanggan grosir. Anda dapat mengunduh kumpulan data menggunakan tautan ini. Data tersebut disimpan di repositori Pembelajaran Mesin UCI.\n",
    "    Tujuan dari masalah ini adalah untuk menyegmentasikan klien distributor grosir berdasarkan pengeluaran tahunan mereka pada berbagai kategori produk, seperti susu, toko bahan makanan, wilayah, dll. Jadi, mari kita mulai membuat kode Kami pertama-tama akan mengimpor perpustakaan yang diperlukan. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-12-8d5bda1be97f>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-12-8d5bda1be97f>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    from kneed import KneeLocator\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "    from kneed import KneeLocator\n",
    "    from sklearn.datasets import make_blobs\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.metrics import silhouette_score\n",
    "    from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data = pd.read_csv('clustering.csv')\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-39c582979392>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"LoanAmount\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"ApplicantIncome\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ApplicantIncome\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"LoanAmount\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'black'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'AnnualIncome'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loan Amount (In Thousands)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "X = data[[\"LoanAmount\",\"ApplicantIncome\"]]\n",
    "plt.scatter(X[\"ApplicantIncome\"],X[\"LoanAmount\"],c='black')\n",
    "plt.xlabel('AnnualIncome')\n",
    "plt.ylabel('Loan Amount (In Thousands)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-da4bd83a79a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Select random observation as centroids\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mCentroids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ApplicantIncome\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"LoanAmount\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'black'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCentroids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ApplicantIncome\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mCentroids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"LoanAmount\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'red'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 1 and 2 - Choose the number of clusters (k) and select random centroid for each cluster\n",
    "\n",
    "#number of clusters\n",
    "K=3\n",
    "\n",
    "# Select random observation as centroids\n",
    "Centroids = (X.sample(n=K))\n",
    "plt.scatter(X[\"ApplicantIncome\"],X[\"LoanAmount\"],c='black')\n",
    "plt.scatter(Centroids[\"ApplicantIncome\"],Centroids[\"LoanAmount\"],c='red')\n",
    "plt.xlabel('AnnualIncome')\n",
    "plt.ylabel('Loan Amount (In Thousands)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-912b3f05973f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mwhile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mXD\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mindex1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrow_c\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mCentroids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 3 - Assign all the points to the closest cluster centroid\n",
    "# Step 4 - Recompute centroids of newly formed clusters\n",
    "# Step 5 - Repeat step 3 and 4\n",
    "\n",
    "diff = 1\n",
    "j=0\n",
    "\n",
    "while(diff!=0):\n",
    "    XD=X\n",
    "    i=1\n",
    "    for index1,row_c in Centroids.iterrows():\n",
    "        ED=[]\n",
    "        for index2,row_d in XD.iterrows():\n",
    "            d1=(row_c[\"ApplicantIncome\"]-row_d[\"ApplicantIncome\"])**2\n",
    "            d2=(row_c[\"LoanAmount\"]-row_d[\"LoanAmount\"])**2\n",
    "            d=np.sqrt(d1+d2)\n",
    "            ED.append(d)\n",
    "        X[i]=ED\n",
    "        i=i+1\n",
    "\n",
    "    C=[]\n",
    "    for index,row in X.iterrows():\n",
    "        min_dist=row[1]\n",
    "        pos=1\n",
    "        for i in range(K):\n",
    "            if row[i+1] < min_dist:\n",
    "                min_dist = row[i+1]\n",
    "                pos=i+1\n",
    "        C.append(pos)\n",
    "    X[\"Cluster\"]=C\n",
    "    Centroids_new = X.groupby([\"Cluster\"]).mean()[[\"LoanAmount\",\"ApplicantIncome\"]]\n",
    "    if j == 0:\n",
    "        diff=1\n",
    "        j=j+1\n",
    "    else:\n",
    "        diff = (Centroids_new['LoanAmount'] - Centroids['LoanAmount']).sum() + (Centroids_new['ApplicantIncome'] - Centroids['ApplicantIncome']).sum()\n",
    "        print(diff.sum())\n",
    "    Centroids = X.groupby([\"Cluster\"]).mean()[[\"LoanAmount\",\"ApplicantIncome\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-6d23c4ea3f02>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'blue'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'green'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'cyan'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Cluster\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ApplicantIncome\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"LoanAmount\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCentroids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ApplicantIncome\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mCentroids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"LoanAmount\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'red'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "color=['blue','green','cyan']\n",
    "for k in range(K):\n",
    "    data=X[X[\"Cluster\"]==k+1]\n",
    "    plt.scatter(data[\"ApplicantIncome\"],data[\"LoanAmount\"],c=color[k])\n",
    "plt.scatter(Centroids[\"ApplicantIncome\"],Centroids[\"LoanAmount\"],c='red')\n",
    "plt.xlabel('Income')\n",
    "plt.ylabel('Loan Amount (In Thousands)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-* Challenges with the K-Means Clustering Algorithm *- \n",
    "    Salah satu tantangan umum yang kami hadapi saat bekerja dengan K-Means adalah ukuran cluster yang berbeda. Cluster paling kiri dan paling kanan berukuran lebih kecil dibandingkan dengan cluster pusat. Sekarang, jika kita menerapkan pengelompokan k-means pada titik-titik ini, hasilnya akan seperti ini. Tantangan lain dengan k-means adalah ketika kepadatan titik awal berbeda. Katakanlah ini adalah poin aslinya.\n",
    "    Di sini, titik-titik di cluster merah tersebar sedangkan titik-titik di cluster yang tersisa saling berdekatan. Sekarang, jika kita menerapkan k-means pada titik-titik ini, kita akan mendapatkan cluster seperti ini. Kita dapat melihat bahwa titik kompak telah ditetapkan ke satu cluster. Sedangkan titik-titik yang tersebar secara longgar tetapi berada di cluster yang sama, telah ditetapkan ke cluster yang berbeda. Tidak ideal jadi apa yang bisa kita lakukan?\n",
    "    \n",
    "   Salah satu solusinya adalah dengan menggunakan jumlah cluster yang lebih banyak. Jadi, dalam semua skenario di atas, daripada menggunakan 3 cluster, kita bisa mendapatkan angka yang lebih besar. Mungkin pengaturan k = 10 dapat menghasilkan cluster yang lebih bermakna.Ingat bagaimana kita menginisialisasi sentroid secara acak di pengelompokan k-means? Nah, ini juga berpotensi bermasalah karena kita mungkin mendapatkan cluster yang berbeda setiap saat. Jadi, untuk mengatasi masalah inisialisasi acak ini, ada algoritma bernama K-Means ++ yang dapat digunakan untuk memilih nilai awal, atau centroid cluster awal, untuk K-Means.\n",
    "    Dalam beberapa kasus, jika inisialisasi cluster tidak sesuai, K-Means dapat menghasilkan cluster yang buruk. Di sinilah K-Means ++ membantu. Ini menentukan prosedur untuk menginisialisasi pusat cluster sebelum melanjutkan dengan algoritma pengelompokan k-means standar.Dengan menggunakan algoritme K-Means ++, kami mengoptimalkan langkah di mana kami memilih sentroid cluster secara acak. Kami lebih cenderung menemukan solusi yang kompetitif untuk solusi K-Means yang optimal saat menggunakan inisialisasi K-Means. \n",
    "    Langkah-langkah untuk menginisialisasi centroid menggunakan K-Means ++ adalah\n",
    "    1. Cluster pertama dipilih secara acak dari titik-titik data yang ingin kita cluster. Ini mirip dengan apa yang kami lakukan di K-Means, tetapi alih-alih memilih semua sentroid secara acak, kami hanya memilih satu sentroid di sini\n",
    "    2. Selanjutnya kita menghitung jarak (D (x)) setiap data point (x) dari center cluster yang sudah dipilih.\n",
    "    3. Kemudian, pilih pusat cluster baru dari titik data dengan probabilitas x sebanding dengan (D (x)) \n",
    "    4. Kemudian ulangi langkah 2 dan 3 sampai k cluster telah dipilih\\n\",\n",
    "    Mari kita ambil contoh untuk memahami ini lebih jelas. Katakanlah kami memiliki poin-poin berikut dan kami ingin membuat 3 kelompok di sini. Sekarang, langkah pertama adalah memilih titik data secara acak sebagai pusat cluster. Misalkan kita memilih titik hijau sebagai sentroid awal. Sekarang, kita akan menghitung jarak (D (x)) dari setiap titik data dengan centroid ini.\n",
    "    Sentroid berikutnya adalah yang jarak kuadratnya (D (x) 2) adalah yang terjauh dari sentroid saat ini.\n",
    "    Dalam hal ini, titik merah akan dipilih sebagai centroid berikutnya. Sekarang, untuk memilih centroid terakhir, kita akan mengambil jarak setiap titik dari centroid terdekatnya dan titik yang memiliki jarak kuadrat terbesar akan dipilih sebagai centroid berikutnya. Kami akan memilih centroid terakhir sebagai Kita dapat melanjutkan algoritme K-Means setelah menginisialisasi sentroid. Menggunakan K-Means ++ untuk menginisialisasi sentroid cenderung meningkatkan cluster. Meskipun secara komputasi mahal relatif terhadap inisialisasi acak, K-Means berikutnya sering kali bertemu lebih cepat.\n",
    "    \n",
    "   Saya yakin ada satu pertanyaan yang Anda tanyakan sejak awal artikel ini - berapa banyak kelompok yang harus kita buat? Aka, berapa jumlah cluster yang harus dimiliki saat menjalankan K-Means? How to Choose the Right Number of Clusters in K-Means Clustering? Salah satu keraguan paling umum yang dimiliki setiap orang saat bekerja dengan K-Means adalah memilih jumlah cluster yang tepat.\n",
    "    Jadi, mari kita lihat teknik yang akan membantu kita memilih nilai cluster yang tepat untuk algoritme K-Means. Mari kita ambil contoh segmentasi pelanggan yang telah kita lihat sebelumnya. Singkatnya, bank ingin menyegmentasikan nasabahnya berdasarkan pendapatan dan jumlah utangnya.\n",
    "    Di sini, kami dapat memiliki dua cluster yang akan memisahkan pelanggan seperti yang ditunjukkan di bawah ini. \n",
    " Semua pelanggan dengan pendapatan rendah berada dalam satu cluster sedangkan pelanggan dengan pendapatan tinggi berada di cluster kedua. Kami juga dapat memiliki 4 cluster. Di sini, satu cluster dapat mewakili pelanggan yang berpenghasilan rendah dan hutang rendah, cluster lainnya adalah tempat pelanggan berpenghasilan tinggi dan hutang tinggi, dan sebagainya. Bisa juga ada 8 cluster.\n",
    "    Sejujurnya, kami dapat memiliki sejumlah cluster. Dapatkah Anda menebak berapa jumlah maksimum kluster yang mungkin? Satu hal yang dapat kita lakukan adalah menetapkan setiap titik ke cluster terpisah. Oleh karena itu, dalam hal ini jumlah cluster akan sama dengan jumlah titik atau pengamatan. Jumlah cluster maksimum yang mungkin akan sama dengan jumlah observasi dalam dataset.\n",
    "    Tapi lalu bagaimana kita bisa menentukan jumlah cluster yang optimal? Satu hal yang dapat kita lakukan adalah memplot grafik, juga dikenal sebagai kurva siku, di mana sumbu x akan mewakili jumlah cluster dan sumbu y akan menjadi metrik evaluasi. Katakanlah inersia untuk saat ini.\n",
    "    Anda juga dapat memilih metrik evaluasi lainnya seperti indeks Dunn. \n",
    "    \"Selanjutnya, kita akan mulai dengan nilai cluster kecil, katakanlah 2. Latih model menggunakan 2 cluster, hitung inersia untuk model tersebut, dan terakhir plot di grafik di atas. Misalkan kita mendapatkan nilai inersia sekitar 1000. \n",
    " \n",
    "   Sekarang, kita akan menambah jumlah cluster, melatih model lagi, dan memplot nilai inersia. Inilah plot yang kami dapatkan. Ketika kami mengubah nilai cluster dari 2 menjadi 4, nilai inersia berkurang sangat tajam. Penurunan nilai inersia ini berkurang dan akhirnya menjadi konstan saat kami menambah jumlah cluster lebih lanjut. nilai cluster dimana penurunan nilai inersia ini menjadi konstan dapat dipilih sebagai nilai cluster yang tepat untuk data kita.\n",
    "    Di sini, kita dapat memilih sejumlah cluster antara 6 dan 10. Kita dapat memiliki 7, 8, atau bahkan 9 cluster. Anda juga harus melihat biaya komputasi sambil menentukan jumlah cluster. Jika kita menambah jumlah cluster, maka biaya komputasi juga akan meningkat. Jadi, jika Anda tidak memiliki sumber daya komputasi yang tinggi, saran saya adalah memilih jumlah cluster yang lebih sedikit.\n",
    "    Sekarang mari kita terapkan algoritme Pengelompokan K-Means dengan Python. Kita juga akan melihat bagaimana menggunakan K-Means ++ untuk menginisialisasi centroid dan juga akan memplot kurva siku ini untuk memutuskan berapa jumlah cluster yang tepat untuk dataset kita.\n",
    "    \n",
    "   -*- Implementing K-Means Clustering in Python -*- \n",
    "    Kami akan menangani masalah segmentasi pelanggan grosir. Anda dapat mengunduh kumpulan data menggunakan tautan ini. Data tersebut disimpan di repositori Pembelajaran Mesin UCI. Tujuan dari masalah ini adalah untuk menyegmentasikan klien distributor grosir berdasarkan pengeluaran tahunan mereka pada berbagai kategori produk, seperti susu, toko bahan makanan, wilayah, dll. Jadi, mari kita mulai membuat kode. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'Wholesale customers data.csv' does not exist: b'Wholesale customers data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-5c30a56c9bc2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# reading the data and looking at the first five rows of the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Wholesale customers data.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'Wholesale customers data.csv' does not exist: b'Wholesale customers data.csv'"
     ]
    }
   ],
   "source": [
    "# reading the data and looking at the first five rows of the data\n",
    "data=pd.read_csv(\"Wholesale customers data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-227901153e31>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# statistics of the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# statistics of the data\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-227901153e31>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# statistics of the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# statistics of the data\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-22c73f581478>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdata_scaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# statistics of scaled data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# standardizing the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# statistics of scaled data\n",
    "pd.DataFrame(data_scaled).describe()\n",
    "kmeans = KMeans(n_clusters=2, init='k-means++')\n",
    "kmeans.fit(data_scaled)\n",
    "kmeans.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_scaled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-dc9ea6326b57>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mkmeans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'k-means++'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mkmeans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_scaled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minertia_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_scaled' is not defined"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=2, init='k-means++')\n",
    "kmeans.fit(data_scaled)\n",
    "kmeans.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_scaled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-395ac66c4f70>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcluster\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mkmeans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_jobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_clusters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcluster\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'k-means++'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mkmeans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_scaled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mSSE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkmeans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minertia_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_scaled' is not defined"
     ]
    }
   ],
   "source": [
    "# fitting multiple k-means algorithms and storing the values in an empty list\n",
    "SSE = []\n",
    "for cluster in range(1,20):\n",
    "    kmeans = KMeans(n_jobs = -1, n_clusters = cluster, init='k-means++')\n",
    "    kmeans.fit(data_scaled)\n",
    "    SSE.append(kmeans.inertia_)\n",
    "\n",
    "# converting the results into a dataframe and plotting them\n",
    "frame = pd.DataFrame({'Cluster':range(1,20), 'SSE':SSE})\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(frame['Cluster'], frame['SSE'], marker='o')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_scaled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-f2ec5678b24d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# k means using 5 clusters and k-means++ initialization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mkmeans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_jobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_clusters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'k-means++'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mkmeans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_scaled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_scaled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_scaled' is not defined"
     ]
    }
   ],
   "source": [
    "# k means using 5 clusters and k-means++ initialization\n",
    "kmeans = KMeans(n_jobs = -1, n_clusters = 5, init='k-means++')\n",
    "kmeans.fit(data_scaled)\n",
    "pred = kmeans.predict(data_scaled)\n",
    "\n",
    "frame = pd.DataFrame(data_scaled)\n",
    "frame['cluster'] = pred\n",
    "frame['cluster'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dengan mencoba dari referensi dari link berikut// https://realpython.com/k-means-clustering-python/ // SAya akan mencoba clustering with k-means "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-26-8d5bda1be97f>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-26-8d5bda1be97f>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    from kneed import KneeLocator\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "    from kneed import KneeLocator\n",
    "    from sklearn.datasets import make_blobs\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.metrics import silhouette_score\n",
    "    from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_blobs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-de0b346f5551>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m features, true_labels = make_blobs(    n_samples=200,\n\u001b[0m\u001b[0;32m      2\u001b[0m         \u001b[0mcenters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mcluster_std\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2.75\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'make_blobs' is not defined"
     ]
    }
   ],
   "source": [
    "features, true_labels = make_blobs(    n_samples=200,\n",
    "        centers=3,\n",
    "        cluster_std=2.75,\n",
    "        random_state=42\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-879bebf25573>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtrue_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mscaled_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mscaled_features\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'features' is not defined"
     ]
    }
   ],
   "source": [
    "features[:5]\n",
    "true_labels[:5]\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "scaled_features[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(\n",
    "        init=\"random\",\n",
    "        n_clusters=3,\n",
    "        n_init=10,\n",
    "        max_iter=300,\n",
    "        random_state=42\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scaled_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-ceca7d7ee6b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mkmeans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscaled_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'scaled_features' is not defined"
     ]
    }
   ],
   "source": [
    "kmeans.fit(scaled_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
